
\subsection{Dataset Information} 


The test data for the graded word sense induction task in SemEval-2013 was drawn from both spoken and written part of the Open American National Corpus (\cite{ide2004openamerican}) and includes 50 terms containing 20 verbs, 20 nouns and 10 adjectives with variety of genres such as fiction, non-fiction, technical. There are a total of 4664 test instances provided. All evaluation was performed on test instances only. In addition, the organizers provided sense labeled trial data which can be used for tuning. This trial data is a redistribution of the Graded Sense and Usage dataset provided by Katrin Erk, Diana McCarthy, and Nicholas Gaylord \cite{erk09graded}. It consists of 8 terms; 3 verbs, 3 nouns, and 2 adjectives all with moderate polysemy (4-7 senses). Each term in trial data has 50 contexts, in total 400 instances provided. Lastly, participants can use ukWaC (\cite{ukWaC}), a 2- billion word web-gathered corpus, for sense induction. Furthermore, unlike in previous WSI tasks, organizers allow participants to use additional contexts not found in the ukWaC under the condition that they submit systems for both using only the ukWaC and with their augmented corpora. \\ 

The gold-standard of test data was prepared using WordNet 3.1 firstly following the work described in \cite{jurgens13embracing} with 20 annotators from Amazon Mechanical Turk, however, after realizing the moderate inter-annotator agreement (IAA), organizers decided to annotate the data by themselves. \\ 

Since WSI systems report their annotations in a different sense inventory than WordNet 3.1, a mapping procedure should be used first. The organizers use the sense mapping procedure explained in \cite{jurgens12evaluation}. This procedure has adopted the supervised evaluation setting  (\cite{agirre06evaluating}; details can be found in Section \ref{section:supervised-eval}) of past SemEval WSI tasks, but the main difference is that the former takes into account applicability weights for each sense which is a necessary for graded word sense. Although the data contains graded senses for some instances, I used only the instances that labeled one sense. This exclusion decreased the number of instances used in the experiments to 4122. Table \ref{table:semeval13-dataset} summarizes the dataset statistics that is used in this section. The following is an example for the test data instance. Note that all reported scores through this thesis are obtained using the single-sense instance set.

\begin{quote}
 $<$instance id="add.v.8" 
 lemma="add" partOfSpeech="v" token="added" tokenEnd="10" tokenStart="5"$>$uh i added up all the taxes that we were going to pay on all these different specific luxury items and travel expenses and everything else
 $<$/instance$>$
\end{quote}

The following section contains baseline scores for the test data on single sense instances.

\begin{table}
\begin{center}
    \begin{tabular}{ l | c | c | c | l }  \Xhline{2\arrayrulewidth}  
     & \bf Noun & \bf Verb & \bf Adjective & \bf All \\  \Xhline{2\arrayrulewidth}  
    \bf \# of Target Words & 20 & 20 & 10 & 50 \\ 
    \bf \# of Instances & 1659 & 1657 & 806 & 4122 \\ 
    \bf Average \# of Sense & 6.5 & 6.15 & 4.8 & 6.02 \\ 
    \bf Average Sense Perplexity & 3.37 & 3.59 & 3.06 & 3.40 \\  \Xhline{2\arrayrulewidth}  
    \end{tabular}
\end{center}
    \caption{\label{table:semeval13-dataset} Dataset statistics for the SemEval 2013 WSID task}
\end{table}

%Evaluation can be divided into two categories: (1) a traditional WSD task for Unsupervised WSD and WSI systems, (2) a clustering comparison setting that evaluates the similarity of the sense inventories for WSI systems. WSD evaluation is made according to three objectives:

\subsection{Baselines}

The following tables depict the performance of the each baseline. Description of the baselines can be found in Section \ref{semeval10-baselines}.


\paragraph{Most Frequent Baseline:} Task organizers provided the FScore of most frequent baseline for single sense data, which is 0.578.

\paragraph{Random Baselines (RB):} Table \ref{table:semeval13-random-baseline} shows the performance of various random baselines. 

\begin{table}
\begin{center}
    \begin{tabular}{  l | l | l | l }
    \Xhline{2\arrayrulewidth}  \bf RB type  & \bf 1 of 2 & \bf 1 of 3 & \bf 1 of n \\ \Xhline{2\arrayrulewidth}
    \bf Induced RB & 0.562 & 0.555 & 0.533 \\ \hline
    \bf RB (uniform) & - & - & 0.533 \\ 
    \bf RB (weighted) & - & - & 0.533 \\ \Xhline{2\arrayrulewidth}
    \end{tabular}
\end{center}
    \caption{\label{table:semeval13-random-baseline} Random baselines on SemEval Task 13 test set. Induced baseline randomly tagged contexts and after Agirre's mapping is applied. For each instance, uniform baseline randomly picks one of the senses in the sense inventory with equal chance. Weighted one considers the sense distribution in the sense inventory.}
\end{table}


\paragraph{SVM-based baselines:} I followed the same procedure as I followed for Semeval 2010 dataset to obtain this baselines with various embeddings in literature and substitute vector extracted from the test set using a 4-gram LM (please see details for the language model and substitute vectors in \emph{TODO: reference}). Table \ref{table:semeval13-svm-baseline} summarizes each embedding's performance, its coverage for the test data.\footnote{Detailed performance table can be seen in \href{https://goo.gl/010sp5}{https://goo.gl/010sp5}}

\begin{table}
\begin{center}
    \begin{tabular}{ r | c | c | c }  \Xhline{2\arrayrulewidth}  
    \bf Representation & \bf \# of word embeddings & \bf Coverage (\%) & \bf Accuracy (\%) \\  \Xhline{2\arrayrulewidth}
    \bf Yatbaz-Y (PoS) & 254746 & 100 & 69.469 \\
    \bf Yatbaz-Y (Global) & 254746 & 100 & 67.823 \\ \hline
    \bf Skipgram & 1119721 & 96.91 & 69.119  \\ 
    \bf Glove & 400001 &  92.49 & 68.476  \\ 
    \bf Multilingual & 180835 & 91.14 & 67.552 \\ 
    \bf Bansal & 316756 & 96.54 & 67.891 \\ 
    \bf Global Context & 100232 & 91.22 & 67.883 \\ 
    \bf CW & 268810 & 97.03 & 68.027 \\ 
    \bf HLBR & 246122 & 96.47 & 68.037 \\ \hline 
    \bf SubDist & - & - & 65.542 \\  \Xhline{2\arrayrulewidth} 
    
    \end{tabular}
\end{center}
    \caption{\label{table:semeval13-svm-baseline} A comparison of the accuracies for various embeddings, and substitute distribution.}
\end{table}


 
\subsection{Comparisons \& Discussion}


In this section, I will describe the systems that were participated in SemEval 2013 Task 13 and a recent system performed in the dataset of the task. At the end, I will provide a table which includes all the systems and the important baselines scores. \\

There were four participant for SemEval-2013 Task 13. Three of them were used induced sense inventory, and the remaining one was an unsupervised WSD system.

% TODO: buraya bi representation referansi ver geriye donuk. Representation section'inda anlattigina bagli olarak.
\paragraph{AI-KU:} \cite{baskaya13ai} represent the context of each target word by using the high probability lexical substitutes according to a statistical language model. A language model is built to identify the relative probabilities of 4-gram sequences and then FastSubs (\cite{fastsubs}) is applied to identify words that appear in the same position as the target word for each context. For example, one instance of bass may have substitutes such as fish, while another instance may have guitar. Each instance is then represented as 100 substitutes, sampled from the probability distribution of the most-probable 100 substitutes for that instance; these substitutes are transformed into a vector representation, reflecting the sampled frequencies of each. The instance-substitute vectors are then projected into a lower dimensionality using S-CODE (\cite{Maron2010}). The final S- CODE based vectors are clustered using k-means. Much like \cite{schutze98automatic}, AI-KU requires specifying the number of clusters ahead of time, often setting k to a larger than necessary number. However, to determine the number of senses, AI-KU performs a post-processing step to remove clusters that contain only a few instances, which are likely artifacts of forcing each of the k clusters to be non-empty. The remaining clusters are treated as senses of the word.

\paragraph{Unimelb:} \cite{lau13word} propose a system that based on a Hierarchical Dirichlet Process (HDP) (\cite{teh06hierarchical}), a nonparametric extention of Latent Dirichlet allocation (\cite{blei03latent}; \cite{steyvers04word}). HDP automatically infers both the number of topics and each topicâ€™s probability distribution for generating the tokens in the corpus. For sense induction, a HDP model is inferred from contexts of a target word, which produces a distribution over topics for each context. Each topic is treated as a distinct sense of the word. Given a new context of the word, the HDP model can be used to infer its topic distribution, thereby identifying which senses are present. For output, we report the full distribution over senses for each context, weighted by their probabilities.

\paragraph{University Of Sussex (UoS):} \cite{hope2013uos} used a graph-based system for the task. First, using depencency-parsed version of UkWaC they construct a graph where vertices are $k$ (they found 300 for the best $k$ for this task) highest ranked words that have dependency relation with the target word. For ranking they use Pointwise Mutual Information measure (\cite{bouma2009pointwise}). Then, they apply MaxMax clustering algorithm (\cite{hope2013maxmax}) and the resulting clusters then merged according to \emph{cohension} and \emph{separation} (\cite{pang2006introduction}) measures.

\paragraph{La Sapienza} was based on applying the Personalized Page Rank algorithm \cite{agirre09personalizing} on WordNet-based network and comparing the each sense with the similarity of the context. After that, each sense is ranked by the similarity.  

\paragraph{ICE and MSSG:} \cite{kageback15neuralembedding} proposed two systems (ICE-online and ICE-kmeans) based on word embeddings and word-context embeddings obtained using the Skip-gram Model (\cite{mikolov2013efficient}; \cite{levy2014dependencybased}) with assigning different weights which assigns uniform weights) to the context words based on the assumption that context words with higher weights express more about the meaning of the target word. After constructing the embeddings, $k$-means clustering algorithm is employed for the former method. In order to choose the $k$ for $k$-means, they used Pham's heuristic (\cite{pham2005selection}). ICE-online is an extension of the Multi-Sense Skip-gram (MSSG) model. Only difference is that in embedding construction part, MSSG (\cite{neelakantan2015efficient}) simply uses uniform weights for all context words around the target words, while former considers the weight and takes into account that some context words are important than others.


\begin{table}
\begin{center}
    \begin{tabular}{  r | c } \Xhline{2\arrayrulewidth}
    \bf Method & \bf F1-Score \\ \Xhline{2\arrayrulewidth} 
    \bf AIKU & 69.469 \\ 
    \bf Unimelb & 69.469 \\ 
    \bf UoS & 69.469 \\ 
    \bf La-Sapienza & 69.469 \\ \hline
    \bf ICE-online & 69.469 \\ 
    \bf ICE-kmeans & 69.469 \\ 
    \bf MSSG & 69.469 \\ \hline
    \bf my-method1 & 69.469 \\ 
    \bf my-method2 & 69.469 \\ \Xhline{2\arrayrulewidth}

    \end{tabular}
\end{center}
    \caption{\label{table:semeval13-system-scores} A comparison of the accuracies for various embeddings, and substitute vectors.}
\end{table}
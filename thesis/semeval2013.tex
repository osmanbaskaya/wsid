
\subsection{Data set Information} 


The test data for the graded word sense induction task in SemEval-2013 includes 50 terms containing 20 verbs, 20 nouns and 10 adjectives. There are a total of 4664 test instances provided. All evaluation was performed on test instances only. In addition, the organizers provided sense labeled trial data which can be used for tuning. This trial data is a redistribution of the Graded Sense and Usage data set provided by Katrin Erk, Diana McCarthy, and Nicholas Gaylord \cite{erk09graded}. It consists of 8 terms; 3 verbs, 3 nouns, and 2 adjectives all with moderate polysemy (4-7 senses). Each term in trial data has 50 contexts, in total 400 instances provided. Lastly, participants can use ukWaC (\cite{ukWaC}), a 2- billion word web-gathered corpus, for sense induction. Furthermore, unlike in previous WSI tasks, organizers allow participants to use additional contexts not found in the ukWaC under the condition that they submit systems for both using only the ukWaC and with their augmented corpora. The gold-standard of test data was prepared using WordNet 3.1 by 10 annotators. Since WSI systems report their annotations in a different sense inventory than WordNet 3.1, a mapping procedure should be used first. The organizers use the sense mapping procedure explained in \cite{jurgens12evaluation}. This procedure has adopted the supervised evaluation setting  (\cite{agirre06evaluating}; details can be found in Section \ref{section:supervised-eval}) of past SemEval WSI tasks, but the main difference is that the former takes into account applicability weights for each sense which is a necessary for graded word sense. Although the data contains graded senses for some instances, I used only the instances that labeled one sense. This exclusion decreased the number of instances used in the experiments to 4122. Table \ref{table:semeval13-dataset} summarizes the data set statistics that is used in this section. The following is an example for the test data instance.

\begin{quote}
 $<$instance id="add.v.8" 
 lemma="add" partOfSpeech="v" token="added" tokenEnd="10" tokenStart="5"$>$uh i added up all the taxes that we were going to pay on all these different specific luxury items and travel expenses and everything else
 $<$/instance$>$
\end{quote}

The following section contains baseline scores for the test data on single sense instances.

\begin{table}
\begin{center}
    \begin{tabular}{ | l | l | l | l | l |}
    \hline  & \bf Noun & \bf Verb & \bf Adjective & \bf All \\ \hline
    \bf \# of Target Words & 10 & 10 & 15 & 10 \\ \hline
    \bf \# of Instances & 10 & 10 & 15 & 10 \\ \hline
    \bf Average \# of Sense & 10 & 10 & 15 & 10 \\ \hline
    \bf Sense Perplexity & 10 & 10 & 15 & 10 \\ \hline
    \bf \# of Annotator & - & - & - & 10 \\ \hline
    \bf Inter-annotator Agreement & - & - & - & \textgreater90\% \\ \hline
    \end{tabular}
\end{center}
    \caption{\label{table:semeval13-dataset} Data set details for the SemEval 2013 WSID task}
\end{table}

%Evaluation can be divided into two categories: (1) a traditional WSD task for Unsupervised WSD and WSI systems, (2) a clustering comparison setting that evaluates the similarity of the sense inventories for WSI systems. WSD evaluation is made according to three objectives:

\subsection{Baselines}

The following tables depict the performance of the each baseline. Description of the baselines can be found in Section \ref{semeval10-baselines}.


\begin{table}
\begin{center}
    \begin{tabular}{ | l | l | l | l | l |}
    \hline  & \bf Noun & \bf Verb & \bf Adjective & \bf All \\ \hline
    \bf \# of Target Words & 10 & 10 & 15 & 10 \\ \hline
    \bf \# of Instances & 10 & 10 & 15 & 10 \\ \hline
    \bf Average \# of Sense & 10 & 10 & 15 & 10 \\ \hline
    \bf Sense Perplexity & 10 & 10 & 15 & 10 \\ \hline
    \bf \# of Annotator & - & - & - & 10 \\ \hline
    \bf Inter-annotator Agreement & - & - & - & \textgreater90\% \\ \hline
    \end{tabular}
\end{center}
    \caption{\label{table:semeval13-baselines}Data set details for the SemEval 2013 WSID task}
\end{table}


\paragraph{Most Frequent Baseline:} Task organizers provided the FScore of most frequent baseline for single sense data, which is 0.578.
\paragraph{Random Baselines:} I used the same procedure to calculate two types of baselines, summarized in \ref{subsection:semeval10}.  \href{http://goo.gl/f2X0da}{Results for random baselines can be seen here}.
\paragraph{kNN-baselines (substitute vectors) :} As explained in \ref{subsection:semeval10}, I followed the same procedure to calculate these baselines and \href{http://goo.gl/J88G7R}{results can be seen in details.}
\paragraph{SVM-based baseline (substitute vectors)} I followed the same procedure as I followed for Semeval 2010 dataset to obtain this baseline. \href{https://goo.gl/010sp5}{Scores for SVM-based baseline can be seen here.}
\paragraph{kNN-baseline (embeddings) :} \href{https://goo.gl/c8F92N}{Scores for embeddings can be seen here.} 

 
\subsection{Comparison with other systems}
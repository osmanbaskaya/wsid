This chapter describes the proposed system for sense discovery problem. First, I would like to present the tools and operations in Section \ref{section:operations}, then clarify their different variations and combinations in Section \ref{section:variations}. The pipeline starts with calculating substitute distribution of a word in a given context explained in Section \ref{section:subs-dist}. Then, a list of substitutes are generated by using this substitute distribution and paired with context id. Context id (e.g., $car_1$) is basically a unique identifier that helps to distinguish each context from others. The euclidean embedding algorithm SCODE uses this co-occurrence data and outputs the embedding for each word. Section \ref{section:scode} clarifies co-occurrence modeling step and Section \ref{section:diff-approaches} defines three types of approaches: local approach, part-of-speech based approach, global approach. Using these word embeddings and the substitute distributions I obtained word representations and context representations which will be clarified in Section \ref{section:representations}. When each context is represented in spherical space, then $k$-means algorithm is employed detailed in Section \ref{section:clustering}.


\section{Tools and Operations}
\label{section:operations}


\subsection{Data Enrichment}
\label{section:enrichment}

Sense annotation is a hard task. For the lexical sample tasks, organizers provide a test data annotated by humans which is used for system evaluation. The systems should use a training data if provided, or corpora for sense induction. Typically test data does not contain enough instance for sense induction. Data enrichment aims to increase the number of instances of target words. Assuming that our target word is $book$ in noun form. I randomly fetch arbitrary number of additional contexts from ukWaC where our target word occurs with the same part-of-speech tag. This implies that I skip those sentences in which the word $book$ functions as a verb. \\

For all experiments in Section \ref{chapter:experiments}, the number of additional contexts is fixed and it is equal to $20000$, and uKWaC is used as a corpus for data enrichment.

\subsection{Substitute Word Distributions}
\label{section:subs-dist}

In this section, I will explain how substitute word distribution captures the paradigmatic relations of word token, and how it is computed. \cite{sahlgren2006word} investigates the syntagmatic and paradigmatic relations captured by the Vector Space Models. His paradigmatic representation models word types using co-occurrence counts. Following the methodology used in \citet{yatbaz2012learning} and \citet{yatbaz2012learning}, I represent the context of word tokens using probabilities of likely substitutes. The substitute word distribution is a probability distribution of observing a word in the context of a word token. Context of a target word in a sentence is defined as the sequence of words in the window of size $2n - 1$ centered at the position of the
target word token. The context excludes the target word token. 


\begin{quote}
``\emph{Accordingly, the last chapter of this \textbf{book} is devoted to our reflections on the impact of channel integration on certain public policies issues.}''
\end{quote}


For instance, in the sentence above, the context of the word token `\emph{book}', when $n = 4$, is ``\emph{chapter of this --- is devoted to}'', where `\emph{---}' denotes the target word token. \\

Assuming the position of the target word token is $0$, the context
spans from positions $-n + 1$ to $n - 1$ and the probability of
observing each type $w$ in our vocabulary in the context of the target
word token is computed using the following equation:
\begin{eqnarray}
  P(w_{0} = w | c_{w_{0}}) & \propto & P(w_{-n + 1} \dots w_{0} \dots w_{n - 1}) \label{eq:ngram1} \\
  & = & P(w_{-n + 1}) \dots P(w_{0} | w_{-n + 1}^{-1}) \dots P(w_{n - 1} | w_{-n + 1}^{n - 2}) \label{eq:ngram2} \\
  & \approx & P(w_{-n + 1}) \dots P(w_{0} | w_{-n + 1}^{-1}) \dots P(w_{n - 1} | w_{0}^{n - 2}) \label{eq:ngram3} \\
  & \propto & P(w_{0} | w_{-n + 1}^{-1}) \dots P(w_{n - 1} | w_{0}^{n - 2}) \label{eq:ngram4}
\end{eqnarray}
Where $w_{i}^j$ is the word sequence from $w_i$ to $w_j$ (for $i < j$)
and $c_{w_{0}}$ is the context of the target word token at position
$0$ of length $2n - 1$, meaning $w_{-n+1}^{-1}$ to the left and
$w_{1}^{n-1}$ to the right of $w_{0}$. \\

In the Equation \ref{eq:ngram1}, the right-hand side is proportional
to the left-hand side because $P(c_{w_{0}})$ is independent of any
given word $w$ for $w_{0}$.  Using the chain rule,
Equation \ref{eq:ngram2} is obtained from the Equation
\ref{eq:ngram1}.  By applying $n^{th}$-order Markov assumption, only the
closest $n - 1$ words in each term of the Equation \ref{eq:ngram2} are
used and others excluded. Result is the Equation \ref{eq:ngram3}.  The Equation \ref{eq:ngram4} is proportional to the Equation \ref{eq:ngram3}
because terms which are do not depend on $w_{0}$ is fixed since the
context of the word at position $0$ is fixed.  Near the boundaries of
the sentence, specifically the first and last $n - 1$ words,
appropriate terms of the Equation \ref{eq:ngram4} are truncated or
dropped (e.g. if $0$ is the first word of a sentence, $P(w_{0} | w_{-n
  + 1}^{-1})$ becomes $P(w_{0})$). \\
  
The probabilities required to compute the Equation \ref{eq:ngram4} can
be obtained from an $n$-gram language model. For this step, I used SRILM (\cite{stolcke02srilm}) to build a 4-gram language model with interpolated Kneser-Ney discounting using the uKWaC, a web corpus with 2 billion tokens. Words that were observed less than 2 times in the language model training data were replaced by <unk> tags, which gave us a vocabulary size of 4,254,946. For computational efficiency only the top 100 substitutes and their probabilities were computed for each position in the context of a target word using the FASTSUBS algorithm (\cite{fastsubs}). These top 100 probabilities were normalized to add up to 1.0 giving us a legitimate probability distribution for a context.

As you seen in Table \ref{table:subs}, the high probability substitutes reflect both semantic and syntactic properties of the context of a target word in the sentence above.


\begin{table}
\begin{center}
    \begin{tabular}{ l | r }  \Xhline{2\arrayrulewidth}  
     \bf Substitute Word & \bf Probability \\  \Xhline{2\arrayrulewidth}
     book & .638 \\
     section & .139 \\
     paper & .089 \\
     volume & .053 \\
     document & .014 \\
     report & .013 \\
     work & .012 \\
     tutorial & .005 \\
     edition & .003 \\
     biography & .001 \\
     \Xhline{2\arrayrulewidth}  
    \end{tabular}
\end{center}
    \caption[Top 10 substitutes and their probabilities for a context]{\label{table:subs}{Top 10 substitutes and their probabilities for the context, ``\emph{chapter of this --- is devoted to}''.}}
\end{table}


\subsection{Discretization of Substitute Word Distribution}
\label{section:sampling}

Although the context of a target word was modeled as a substitute word distribution, the Euclidean co-occurrence modeling algorithm which will be introduced in the next section expects its input as discrete objects. I will list two methods to discretize substitute word distribution into discrete counterpart. \cite{sert2013word} proposed a method which creates proximity matrix of substitute word distributions, and  ids of $k$-nearest neighbors of each distribution. The second method introduced in \cite{yatbaz2012learning} draws $k$ word types with replacement from the substitute word distributions. This method is computationally cheaper than the former one, however, the number of samples should not be small and requires attention to pick. Too small $k$ values make this setting not to represent the distribution, however, too large $k$ values make the following steps computationally expensive. From my preliminary  experiments, 100 samples enough for each context. Thus, for each context, I sampled 100 word types from their correspondent substitute distributions.

\subsection{Co-occurrence Modeling with S-CODE algorithm}
\label{section:scode}

In this section, I give a brief introduction for the Symmetric Interaction Model of the Co-occurrence Data Embedding (CODE, \cite{globerson2004euclidean}) and its extension Spherical Co-Occurrence Data Embedding (S-CODE, \cite{Maron2010}). S-CODE randomly initializes co-occurrence data generated using substitute word distributions and discretization operation explained in the previous section into $d$ dimensional Euclidean sphere. After the algorithm converges, the contexts with similar substitute word distributions are close to each other. In Section \ref{section:clustering}, I will explain how S-CODE output used, and how it combined with substitute word distributions in order to obtain high quality context embeddings. \\ 

Let $X$ and $Y$ be two categorical variables with finite cardinality
$|X|$ and $|Y|$.  We observe a set of pairs $\{x_i, y_i\}_{i=1}^n$
drawn IID from the joint distribution of $X$ and $Y$.  These pairs are
summarized by the empirical distributions $\bar{p}(x,y)$, $\bar{p}(x)$
and $\bar{p}(y)$. The task is to find embeddings $\phi(x)$ and
$\psi(y)$ for each unique variable $x$ and $y$. These embeddings require to reflect the statistical relationship between the variables $x$ and $y$ such that pairs occur together frequently should close to each other. hence, the square of Euclidean distance, \mbox{$d^2_{x,y} = |\phi(x) - \psi(y)|^2$}, should be small in $d$ dimensional space. \\

Later, \citet{Maron2010} constrain the embedding to lie on a high dimensional
unit sphere so that enables to approximate the partition function which is a dynamic variable, $Z$, using a constant, $\bar{Z}$, which arises from a coarse approximation in which all pairs of embedded variables are distributed uniformly and independently on the sphere. This constraint allows to work on large datasets by avoiding to calculate $Z$ after each gradient step. Thus, in this thesis I used Spherical CODE:

\begin{equation} \label{eq:marginal-marginal-model}
  p(x,y) = \frac{1}{Z} \bar{p}(x) \bar{p}(y) e^{-d^2_{x,y}}
\end{equation}
where $Z = \sum_{x,y} \bar{p}(x) \bar{p}(y) e^{-d^2_{x,y}}$ is the
normalization term.  We can express the log-likelihood of the joint
distribution over all embeddings $\phi$ and $\psi$ as the following:

\begin{eqnarray} \label{eq:log-likelihood}
  \ell(\phi,\psi) & = & \sum_{x,y} \bar{p}(x,y) \log p(x,y) \nonumber \\
  & = & \sum_{x,y} \bar{p}(x,y) (-\log Z + \log \bar{p}(x) \bar{p}(y) - d^2_{x,y}) \nonumber \\
  & = & -\log Z + const - \sum_{x,y} \bar{p}(x,y) d^2_{x,y}
\end{eqnarray}

The gradient of the log-likelihood depends on the sum of embeddings
$\phi(x)$ and $\psi(y)$, for $x \in X$ and $y \in Y$, and to maximize
the log-likelihood, \cite{Maron2010} use a gradient-ascent
approach. The gradient is as follows:

\begin{equation} \label{eq:gradient-phi}
  \frac{\partial \ell(\phi,\psi)}{\partial \phi(x)} =%
  \sum_{y} 2 \bar{p}(x,y) {[}\psi(y) - \phi(x){]} +%
  \frac{1}{Z} \sum_{y} \bar{p}(x) \bar{p}(y) {[}\phi(x) - \psi(y){]} e^{-d^2_{x,y}}
\end{equation}

\begin{equation} \label{eq:gradient-psi}
  \frac{\partial \ell(\phi,\psi)}{\partial \psi(y)} =%
  \sum_{x} 2 \bar{p}(x,y) {[}\phi(x) - \psi(y){]} +%
  \frac{1}{Z} \sum_{x} \bar{p}(x) \bar{p}(y) {[}\psi(y) - \phi(x){]} e^{-d^2_{x,y}}
\end{equation}

The first sum, the gradient of the part with $d^2_{x,y}$ in
(\ref{eq:log-likelihood}), in (\ref{eq:gradient-phi})
{[}(\ref{eq:gradient-psi}){]} acts attraction force between the
$\phi(x)$ ($\psi(y)$) and all the embeddings $\psi$ ($\phi$) in
proportion to respective joint empirical probabilities $\bar{p}(x,y)$. The second sum, the gradient of $-\log Z$ in
(\ref{eq:log-likelihood}), in (\ref{eq:gradient-phi})
{[}(\ref{eq:gradient-psi}){]} acts a repulsion force between the
$\phi(x)$ ($\psi(y)$) and all the embeddings $\psi$ ($\phi$) in
proportion to respective marginal empirical probabilities $\bar{p}(x)$
and $\bar{p}(y)$. \\

In this work, I use S-CODE with sampling based stochastic gradient ascent, a constant approximation of $Z$ (which is 0.166) and randomly initialized $\phi$ and $\psi$ vectors in 100 dimensional sphere ($d = 100$). The algorithm stops if the difference between two consecutive likelihood values is less than 0.0001.


\subsection{Clustering}
\label{section:clustering}

In this thesis, I tested three different clustering algorithms: Weighted $k$-means which is $k$-means algorithm with (optional) instance weights with careful initialization (\cite{arthur2007k})\footnote{An efficient implementation can be found in https://github.com/ai-ku/wkmeans}. In literature, there are two common methods to specify $k$, required parameter for $k$-means algorithm: \emph{Gap Statistics} (\cite{tibshirani00estimating}) and Phamâ€™s heuristic (\cite{pham2005selection}). \\

I also tried Spherical Clustering (\cite{shi2000normalized}) and DBSCAN (\cite{ester1996density}). The representations I used for clustering will be explain in the following section \ref{section:representations} and the performance of these algorithms will compare in Chapter \ref{chapter:experiments}.


\section{Variations and combinations}
\label{section:variations}

In the previous section I explained the pipeline of operations I used for Word Sense Induction problem. In this section, I will explain the combinations of operations and their variations. In Section \ref{section:scode} we successfully obtain embeddings that reflect the statistical relations by using S-CODE algorithm. The following section describes how can we use these embeddings to acquire context embeddings. After that, in Section \ref{section:diff-approaches} I will discuss local, POS-based, and global approach for the co-occurrence modeling step and the clustering step. 


\subsection{Different Representations}
\label{section:representations}



\subsection{Different approaches}
\label{section:diff-approaches}

The following three approaches tested in the thesis:

\paragraph{Local approach:} In the local approach, clustering is employed for each target word, separately. In other words, 

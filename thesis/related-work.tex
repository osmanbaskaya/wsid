In this chapter, I will summarize the different techniques and representations on Word Sense Induction. \\

Firstly, there are two approaches to extract senses of a word: local and global. Global algorithms (also known as \emph{word clustering algorithms}) try to discriminate word senses using the other words. For instance, Clustering-by-Committee (CBC) algorithm proposed in  \cite{pantel02discovering} starts discovering a set of tight (unambiguous) clusters where each centroid of the clusters are called \emph{committee}. Remaining words can be assigned one of the closest committee and the key idea here is that after a word assigns to a committee, feature vector of assigned word is subtracted from feature vector of committee. This may provide to discover less dominant sense of the words. On the other hand, local algorithms aim to discover senses of a word separately representing the context for each occurrence and clustering those contexts (\cite{schutze98automatic}). Local clustering algorithms can be divided into two groups: context clustering algorithms and graph-based algorithms. Context clustering algorithms represents the context of a word by co-occurrence statistics. Then, clustering methods can be employed to those context vectors. Each cluster denotes the different sense of the word in question. \\

The frequencies and counts is used to represent queries, documents as vector was introduced by \cite{salton75vector}. Vector Space Models (\cite{turney10frequency}) gained a lot of attention for semantic information. The premise is that documents are similar if they consist of the same words. The distributional hypothesis of the semantic says that words that are used and occur in the same contexts tend to bear similar meanings. This idea is popularized by the phrase "You shall know the word by company it keeps." (\cite{firth57synopsis}). One of the extensive work on first-order and the second-order features was proposed by \cite{schutze98automatic}. The first-order features denote the immediate neighbors of a target word. That is, the set of words co-occur with a target word. On the other hand, second-order features (second-order co-occurrences) can be think of friends-of-friends relationship: The set of words co-occur with not only target word but also co-occuring words with the target word. \cite{purandare04discriminating} analyzes word sense discrimination techniques that are used first-order and second-order features.  Another method which is based on representation of context as term (token) vector is that Latent Semantic Analysis (LSA) (\cite{deerwester90indexing}; \cite{landauer1997solution}). LSA uses term-document (context) matrix where each cell contains frequency of each term in a particular document. After computing these frequencies, singular value decomposition (SVD) method is employed. This method provides three matrix and the important latent dimensions in order. Using $k$ latent dimensions, original frequency matrix is mapped into dense and lower dimensional counterpart. This enables computational efficiency and it helps to combat with the sparsity of the original matrix. \cite{vandecruys11latent} proposed an unified model based on LDA and the system outperformed the systems participated in SemEval-2010 word sense induction and disambiguation task. Graph-based algorithms also utilized co-occurrence statistics. Typically, semantic of a word is represented by a co-occurrence graph whose vertices bear co-occurrence information and edges hold the co-occurrence statistics. \cite{veronis04hyperlex} proposed HyperLex, graph based algorithm, uses all words (i.e., global approach), uses a specific graph property called "small-world" which means that graph has highly interconnected groups (hubs) unlike random graphs, and these hubs eventually denote the senses of a word. \cite{agirre2006two} extended their previous work in \cite{agirre06evaluating}  by adapting the PageRank algorithm \cite{page1999pagerank} compared their system with HyperLex on Senseval-3 Lexical sample and all words tasks. UoY is another graph-based algorithm participated in SemEval-2010 WSI task and proposed in \cite{korkontzelos10uoy}. UoY constructs a graph with units as nodes. A unit can be either a single word if it is predicted as unambiguous, or word groups assuming that word groups represent only one sense of a word (\cite{klapaftis08word}). Then, Chinese Whispers (\cite{biemann06chinese}) runs on constructed graph. They achieved relatively high scores in the task. UoS proposed in \cite{hope2013uos} and their difference is that they constructed a dependency-parsed graph using uKWaC, a web-based corpus. They used Pointwise Mutual Information measure (\cite{bouma2009pointwise}) while creating the graph. MaxMax (\cite{hope2013maxmax}) is used to cluster the graph. Furthermore, they merged the clusters according to \emph{cohension} and \emph{separation} (\cite{pang2006introduction}) measures. \cite{jurgens11word} used the graph approach posed the problem as community detection problem, a well studied problem in network science. Since using the all co-occurrence information is prohibitively expensive, author created a representative subset of the original. Then, a co-occurrence graph was built with iterative process. Each co-occurrence of a term with context an edge created if there is none, otherwise, its weight was increased by one. Edges whose weights are below than 25 were pruned. Then, Link Clustering (\cite{ahn10link}) is used on the graph for community detection and achieved competitive performance on the dataset provided in SemEval 2010 WSI task. \\

Representing words as low dimensional, dense vectors is increased popularity for many problems in NLP. There are two ways of obtaining word embeddings. One approach is to use co-occurrence information directly. Using frequency matrix where rows and columns are events, cells denote how many times two events occur simultaneously (e.g., how many times word $i$ appears with word $j$ in a particular context such as context window, sentence, paragraph, document). Since raw counts of words that are occur in the same context may not be informative, there are methods to convert the frequency matrix into tf-idf (term frequency - inverted document frequency) (\cite{sparck1972statistical}) or Pointwise Mutual Information (PMI, \cite{church1990word}) counterparts. Another common approach to reduce the high dimensionality and deal with the sparsity of frequency matrix is matrix factorization techniques explained above. \cite{turney10frequency} gives more detailed analysis regarding Vector Space Models (VSM) and its application for semantics. More recent methods are utilization of neural networks. There are high volume of methods proposed for embedding words. For example, \cite{collobert2008unified} proposed an unified model based on convolutional neural network which predicts part-of-speech tags, chunks, named entity tags using labeled data, and likelihood of a sentence, representation of a word type using unlabeled data. \cite{mnih2009scalable} present a hierarchical language model based on \cite{morin2005hierarchical}, which learns representation of words. Feedforward neural network language models have certain limitations such as the requirement to specify the context length. \cite{mikolov2010recurrent} used \emph{simple recurrent neural network} to overcome this problem, and showed significant performance achievement over state-of-the-art backoff models. As a byproduct, the neural network also learns word representations. These word embeddings contain syntactic and semantic information (\cite{mikolov2013linguistic}). \cite{mikolov2013efficient} proposed efficient architectures to compute continuous word representations. \cite{globerson2004euclidean} described another method called CODE for embedding objects such as word in a single Euclidean space based on their co-occurrence statistics. Spherical CODE (SCODE, \cite{Maron2010}) constrained CODE method so that embeddings lie on a high-dimensional unit sphere which provided an efficient optimization on large datasets and high dimensional embeddings. \cite{yatbaz2012learning} extended this method using most probable substitutes (paradigmatic relations; see Figure \ref{fig:paradigmatic}) obtained by a language model and achieved best results on unsupervised part-of-speech induction problem. Similar method is used in \cite{baskaya13ai} with a crucial difference: Context is represented by most probable substitutes of each target word and their clusters decided the sense of the target word. This difference addressed the ambiguity. \cite{cirik2014substitute} investigated the substitute based SCODE word embeddings on supervised tasks such as Named Entity Recognition (NER), Chunking, and Dependency Parsing. \cite{yatbaz2014unsupervised} proposed a method which solved the limitation of the previous method (\cite{yatbaz2012learning}) by representing not only word but also the context. This instance-based method evaluated with 19 different corpora in 15 languages and achieved results which were significantly better than or comparable to the best
published word or instance based systems. \cite{sert2013word} present a comprehensive description for word (type) and context (token) embeddings obtained by SCODE. \\


\cite{brody09bayesian} proposed a Bayesian approach to model the contexts of the ambiguous word as samples from a multinomial distribution over senses which are in turn characterized as distribution over words. Their work is related to latent Dirichlet allocation (LDA, \cite{blei03latent}). Difference is that while LDA generates word from \emph{global topics}, they generate words from \emph{local topics} which are chosen based on the words observed in a context window around the ambiguous word. Their work is also categorized as local approach because they built a model for each word in question. The model also considers the part-of-speech and dependency information. They outperformed the systems participated in SemEval 2007 sense induction and discrimination task (\cite{agirre07semeval}). \cite{yao11nonparametric} proposed a nonparametric Bayesian Model, Hierarchical Dirichlet Allocation (HDP) for Word Sense Induction. The main drawback of LDA is that number of senses of each word in question is specified as a parameter beforehand. Nonparametric methods (\cite{reisinger2010mixture}; \cite{vlachos2009unsupervised}), however, make this decision automatically. They evaluated their system using supervised framework proposed in \cite{agirre07semeval}, and provided a comparison with LDA based model suggested in \cite{brody09bayesian}. The training data is British National Corpus \cite{clear1993british} and the test data they used is SemEval-2007 WSI. Their model which identified sense granularity  performed equivalent or better than \cite{brody09bayesian}. Another HDP-based model is \cite{lau13word}, and achieved competitive performance in SemEval-2013 WSI task. \\

Frequent termsets mining aims to reveal patterns (termsets) that are observed frequently in data. First, \cite{rybinski2008discovering} proposed a text mining approach based on frequent termsets. This method is domain and language independent, however, it requires part-of-speech information. The method consists in determining atomic contexts of
terms of interest by means of maximal frequent termsets, which then are used for determining discriminant contexts. Recently, SnS (Sense Search) was proposed by \cite{kozlowski2014sns}. This algorithm takes a textual corpus, the target word, and similarity threshold values. First, it discovers contextual patterns from the context. Context consists of proper names, nouns and named entities processed by a part-of-speech tagger and a named entity recognizer. This pattern discovery is made by CHARM algorithm (\cite{zaki2002charm}), an efficient algorithm for mining closed frequent itemsets. With these contextual patterns SnS extracts sense frames. Then, it clusters these sense frames.

\begin{figure}
\centering
  \includegraphics[width=0.5\textwidth]{images/paradigmatic.png}
  \caption[Syntactic and paradigmatic axes]{Syntactic and paradigmatic axes of \emph{the man cried} (\cite{chandler2007semiotics})}
  \label{fig:paradigmatic}
\end{figure}


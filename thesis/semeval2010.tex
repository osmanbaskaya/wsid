
\emph{TODO: LM details should be explained somewhere above.}


\subsubsection{Dataset} 
The test dataset is part of OntoNotes \cite{hovy06ontonotes}. The texts come from various news sources including CNN, ABC and others. The test set for this task consists of 100 words, 50 nouns and 50 verbs; 5285 noun instances and 3630 verb instances, 8915 instances in total. Average number of sense for nouns is 4.46 and for verbs is 3.12. The following is an example for the test data instance.

\begin{quote}
  $<swim.v.1>$ \\
 First of all , visibility will be very very low . $<TargetSentence>$ It 
 's going to be bitterly cold , and there is going to be enormous 
 danger from jagged pieces of metal which could be swimming around in
 the submarine . $</TargetSentence>$ Given the fact that there are so many
 dangers and that these divers are risking their own lives , I wonder
 if there is consideration given to the fact that this may not be 
 really worth it . \\
 $</swim.v.1>$
\end{quote}



%I removed 87 instances because of two reasons. Some target word was observed 

\subsubsection{Baselines}

\paragraph{Most Frequent Baseline:} Task organizers provided the FScore of most frequent baselines (MFS), which are 0.532, 0.666 and 0.587 for nouns, verbs, and all words, respectively.
\paragraph{Random Baselines:}I calculated four types of random baselines: (1) Random induced sense baseline, (2) uniform random baseline (3) shuffled version of random baseline, and (4) the random baseline uses the same sense distribution with the gold data. The first one is a dummy system that provides random induced senses for each instance. Since this random baseline provides arbitrary senses (i.e., the sense inventory is not the same with the gold standard sense inventory), the mapping between these induced senses and gold standard senses needs to be done. The number in the name of the baseline indicates how many different induced senses are provided for each target word. The second type of random baseline uses the correct number of sense for each target word and picks a sense among those senses provided in gold standard with same chance (uniformly distributed). Third one uses the gold standard and shuffles them. Then, it assigns shuffled senses to the instances. That is, each type of  senses for this baseline equals to the gold standard's. The last baseline picks senses according to the sense distribution of each word. \href{http://goo.gl/f2X0da}{Results for random baselines can be seen here}.
\paragraph{kNN-baselines (substitute vectors) :} These baselines are computed as follows. First, the most frequent 100 substitutes and their probabilities are found for each test instance using \emph{FASTSUBS} algorithm \cite{fastsubs} and a language model that built by using ukWaC \cite{ukWaC} as corpus and SRILM \cite{stolcke02srilm} as a language model library. These 100 substitutes is not a probability distribution and needs to be normalized. After normalization, I obtained legitimate probability distributions and each instance is represented by its substitute distribution. Using various distance metrics (euclid, cosine, manhattan, maximum, jensen), I found the closest neighboring test instances and their distances for each instance. The two types of kNN baseline are calculated: \emph{majority voting} and \emph{minumum average distance}. The first type is usual kNN. Using the answers (gold standard for test data), it decides the sense of the current instance by looking labeled senses of $k$ neighboring instances. This version does not consider the distance values. That is, the weights of the each neighbors are equal in sense deciding process. It returns the majority sense as the predicting sense. The other baseline differs from the first and it takes into account the distance between the neighbors and the instance whose sense is in question. It returns the sense that has the minimum average distance among the senses that $k$ closest neighbors of the instance have. \href{http://goo.gl/ofm4cW}{Results can be seen in details}.
\paragraph{kNN-baseline (embeddings) :} Two types of representation are tested here. One is $X\bar{Y}$ and other is $XY_w$ (see Section \ref{section:representations} for details). \href{https://goo.gl/010sp5}{Scores for embeddings can be seen here.} 
\paragraph{SVM-based baseline (substitute vectors) :} This baseline is similar to kNN baseline; SVM with RBF kernel is used as classifier. Hyperparameters are optimized by grid search and scores are obtained by 5-fold cross validation. \href{https://goo.gl/i43T1U}{Results and best hyperparameter sets can be found here}.
\paragraph{SVM-based baseline (with XY$_w$ embeddings) :} Scores of XY$_w$ embeddings obtained Pos-based co-occurrence modeling can be found \href{https://goo.gl/Zofhfm}{here}. \\


There are many embeddings in literature that are obtained with various methods. I also provided four more baselines utilized different embedding sets (Details of these embeddings can be found Subsection \ref{subsection:embeddings}). Note that these baselines use SVM as a classifier. However, difference of embedding vector of each word makes each baseline different. These representations consist of two parts as XY$_w$ word embeddings. First part (X) of each representation is the same for all four embeddings and it is obtained by POS-based co-occurrence modeling (more details in Section \ref{section:representations}. The second part (Y$_w$) of the representation comes from weighted average of the embedding vectors seen in top 100 substitutes of the context. Any substitute word that is absent in embedding set skipped, and the normalization is done for remaining substitute words. \href{https://goo.gl/Tcym7B}{The coverage of each embedding sets for these baselines can be found here}.

The probabilities of each substitute word for given context is used as weight of its embedding vector. In other words, the embedding vector of each substitute word contributes according to its observed probability for the given context. Remember that this probabilities calculated by using 4-gram language model that built from uKWaC, a 2- billion word web-gathered corpus.  \\

\href{https://goo.gl/Tcym7B}{Scores of the different embeddings can be seen here.}

